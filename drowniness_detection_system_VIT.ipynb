{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3667213,"sourceType":"datasetVersion","datasetId":2195166}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nimport shutil\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:17.080983Z","iopub.execute_input":"2025-02-09T18:24:17.081296Z","iopub.status.idle":"2025-02-09T18:24:30.105682Z","shell.execute_reply.started":"2025-02-09T18:24:17.081260Z","shell.execute_reply":"2025-02-09T18:24:30.104783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:36.865480Z","iopub.execute_input":"2025-02-09T18:24:36.866082Z","iopub.status.idle":"2025-02-09T18:24:37.535148Z","shell.execute_reply.started":"2025-02-09T18:24:36.866052Z","shell.execute_reply":"2025-02-09T18:24:37.534356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Set memory growth to prevent TensorFlow from pre-allocating all GPU memory\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPU is configured successfully!\")\n    except RuntimeError as e:\n        print(e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:38.901774Z","iopub.execute_input":"2025-02-09T18:24:38.902097Z","iopub.status.idle":"2025-02-09T18:24:38.907666Z","shell.execute_reply.started":"2025-02-09T18:24:38.902072Z","shell.execute_reply":"2025-02-09T18:24:38.906679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths\ndata_dir = \"/kaggle/input/driver-drowsiness-dataset-ddd/Driver Drowsiness Dataset (DDD)\"  # Update this to your dataset path\ndrowsy_dir = os.path.join(data_dir, 'Drowsy')\nnon_drowsy_dir = os.path.join(data_dir, 'Non Drowsy')\n\n# Ensure dataset directories exist\nassert os.path.exists(drowsy_dir), \"Drowsy folder not found\"\nassert os.path.exists(non_drowsy_dir), \"Non-drowsy folder not found\"\n\n# Create train, val, test directories\noutput_dir = \"prepared_data\"\ntrain_dir = os.path.join(output_dir, \"train\")\nval_dir = os.path.join(output_dir, \"val\")\ntest_dir = os.path.join(output_dir, \"test\")\n\nfor split in [train_dir, val_dir, test_dir]:\n    for category in [\"drowsy\", \"non_drowsy\"]:\n        os.makedirs(os.path.join(split, category), exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:40.255430Z","iopub.execute_input":"2025-02-09T18:24:40.255778Z","iopub.status.idle":"2025-02-09T18:24:40.275911Z","shell.execute_reply.started":"2025-02-09T18:24:40.255741Z","shell.execute_reply":"2025-02-09T18:24:40.275237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function to split and copy files\ndef split_and_copy(source_dir, target_dirs, split_ratios):\n    filenames = os.listdir(source_dir)\n    train, test = train_test_split(filenames, test_size=split_ratios[1] + split_ratios[2], random_state=42)\n    val, test = train_test_split(test, test_size=split_ratios[2] / (split_ratios[1] + split_ratios[2]), random_state=42)\n    \n    splits = {\"train\": train, \"val\": val, \"test\": test}\n    for split_name, files in splits.items():\n        split_path = target_dirs[split_name]\n        category_name = os.path.basename(source_dir)  # 'Drowsy' or 'Non Drowsy'\n        category_path = os.path.join(split_path, category_name.lower().replace(\" \", \"_\"))  # Convert to lowercase and underscores\n        \n        # Create subdirectory if it doesn't exist\n        os.makedirs(category_path, exist_ok=True)\n        \n        for file in files:\n            shutil.copy(os.path.join(source_dir, file), os.path.join(category_path, file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:41.873748Z","iopub.execute_input":"2025-02-09T18:24:41.874047Z","iopub.status.idle":"2025-02-09T18:24:41.880476Z","shell.execute_reply.started":"2025-02-09T18:24:41.874023Z","shell.execute_reply":"2025-02-09T18:24:41.879419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_and_copy(drowsy_dir, {\"train\": train_dir, \"val\": val_dir, \"test\": test_dir}, [0.7, 0.2, 0.1])\nsplit_and_copy(non_drowsy_dir, {\"train\": train_dir, \"val\": val_dir, \"test\": test_dir}, [0.7, 0.2, 0.1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:24:44.166635Z","iopub.execute_input":"2025-02-09T18:24:44.166933Z","iopub.status.idle":"2025-02-09T18:31:34.218772Z","shell.execute_reply.started":"2025-02-09T18:24:44.166909Z","shell.execute_reply":"2025-02-09T18:31:34.217988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ImageDataGenerator for data loading and augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=20, zoom_range=0.15, width_shift_range=0.2,\n                                   height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\nval_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\nval_generator = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:31:41.795020Z","iopub.execute_input":"2025-02-09T18:31:41.795357Z","iopub.status.idle":"2025-02-09T18:31:42.967219Z","shell.execute_reply.started":"2025-02-09T18:31:41.795335Z","shell.execute_reply":"2025-02-09T18:31:42.966579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:31:45.978231Z","iopub.execute_input":"2025-02-09T18:31:45.978571Z","iopub.status.idle":"2025-02-09T18:31:45.982736Z","shell.execute_reply.started":"2025-02-09T18:31:45.978511Z","shell.execute_reply":"2025-02-09T18:31:45.981753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass ClassToken(layers.Layer):\n    def __init__(self, d_model):\n        super().__init__()\n        self.d_model = d_model\n        \n    def build(self, input_shape):\n        self.class_token = self.add_weight(\n            shape=(1, 1, self.d_model),\n            initializer='zeros',\n            trainable=True,\n            name='class_token'\n        )\n        \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        class_token = tf.tile(self.class_token, [batch_size, 1, 1])\n        return tf.concat([class_token, inputs], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:32:00.426667Z","iopub.execute_input":"2025-02-09T18:32:00.426972Z","iopub.status.idle":"2025-02-09T18:32:00.432402Z","shell.execute_reply.started":"2025-02-09T18:32:00.426948Z","shell.execute_reply":"2025-02-09T18:32:00.431447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionEmbedding(layers.Layer):\n    def __init__(self, num_patches, d_model):\n        super().__init__()\n        self.num_patches = num_patches\n        self.d_model = d_model\n        \n    def build(self, input_shape):\n        self.position_embeddings = self.add_weight(\n            shape=(1, self.num_patches + 1, self.d_model),\n            initializer='zeros',\n            trainable=True,\n            name='position_embeddings'\n        )\n        \n    def call(self, inputs):\n        return inputs + self.position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:32:12.370462Z","iopub.execute_input":"2025-02-09T18:32:12.370816Z","iopub.status.idle":"2025-02-09T18:32:12.375923Z","shell.execute_reply.started":"2025-02-09T18:32:12.370790Z","shell.execute_reply":"2025-02-09T18:32:12.375015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_vit_model(input_shape=(224, 224, 3), num_classes=2, \n                    d_model=64, num_heads=4, num_layers=4, \n                    mlp_dim=128, patch_size=16):\n    num_patches = (input_shape[0] // patch_size) ** 2  \n    inputs = layers.Input(shape=input_shape)\n    \n    x = layers.Conv2D(d_model, patch_size, strides=patch_size, name='patch_embedding')(inputs)\n    x = layers.Reshape((num_patches, d_model))(x)\n    \n    x = ClassToken(d_model)(x)\n    \n    x = PositionEmbedding(num_patches, d_model)(x)\n    \n    for _ in range(num_layers):\n        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n        \n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=d_model//num_heads\n        )(x1, x1)\n        \n        x = layers.Add()([x, attention_output])\n        \n        x2 = layers.LayerNormalization(epsilon=1e-6)(x)\n        \n        mlp_output = layers.Dense(mlp_dim, activation='gelu')(x2)\n        mlp_output = layers.Dense(d_model)(mlp_output)\n        \n        x = layers.Add()([x, mlp_output])\n    \n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    x = x[:, 0, :]  \n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    return Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:32:33.855009Z","iopub.execute_input":"2025-02-09T18:32:33.855452Z","iopub.status.idle":"2025-02-09T18:32:33.867553Z","shell.execute_reply.started":"2025-02-09T18:32:33.855415Z","shell.execute_reply":"2025-02-09T18:32:33.866473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nvit_model = create_vit_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:33:08.834133Z","iopub.execute_input":"2025-02-09T18:33:08.834466Z","iopub.status.idle":"2025-02-09T18:33:10.830209Z","shell.execute_reply.started":"2025-02-09T18:33:08.834440Z","shell.execute_reply":"2025-02-09T18:33:10.829503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vit_model.compile(optimizer='adam',\n                 loss='categorical_crossentropy',\n                 metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:33:15.486814Z","iopub.execute_input":"2025-02-09T18:33:15.487104Z","iopub.status.idle":"2025-02-09T18:33:15.500892Z","shell.execute_reply.started":"2025-02-09T18:33:15.487082Z","shell.execute_reply":"2025-02-09T18:33:15.499683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = vit_model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T18:33:33.547814Z","iopub.execute_input":"2025-02-09T18:33:33.548122Z","iopub.status.idle":"2025-02-09T19:38:48.049740Z","shell.execute_reply.started":"2025-02-09T18:33:33.548098Z","shell.execute_reply":"2025-02-09T19:38:48.049016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vit_model.save(\"drowsiness_detection_vit_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:39:18.871450Z","iopub.execute_input":"2025-02-09T19:39:18.871819Z","iopub.status.idle":"2025-02-09T19:39:19.047137Z","shell.execute_reply.started":"2025-02-09T19:39:18.871788Z","shell.execute_reply":"2025-02-09T19:39:19.046221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(vit_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Apply default optimizations\ntflite_model = converter.convert()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:40:23.943215Z","iopub.execute_input":"2025-02-09T19:40:23.943509Z","iopub.status.idle":"2025-02-09T19:40:27.675256Z","shell.execute_reply.started":"2025-02-09T19:40:23.943484Z","shell.execute_reply":"2025-02-09T19:40:27.674257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('drowsiness_detection_vit_model_tflite.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:41:03.751908Z","iopub.execute_input":"2025-02-09T19:41:03.752250Z","iopub.status.idle":"2025-02-09T19:41:03.756680Z","shell.execute_reply.started":"2025-02-09T19:41:03.752221Z","shell.execute_reply":"2025-02-09T19:41:03.755867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_generator = val_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\ntest_loss, test_acc = vit_model.evaluate(test_generator)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T19:41:46.708758Z","iopub.execute_input":"2025-02-09T19:41:46.709062Z","iopub.status.idle":"2025-02-09T19:42:02.582272Z","shell.execute_reply.started":"2025-02-09T19:41:46.709038Z","shell.execute_reply":"2025-02-09T19:42:02.581457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}